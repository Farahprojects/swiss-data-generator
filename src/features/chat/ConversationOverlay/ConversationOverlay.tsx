import React, { useCallback, useEffect, useRef, useState } from 'react';
import { createPortal } from 'react-dom';
import { useConversationUIStore } from '@/features/chat/conversation-ui-store';
import { useChatStore } from '@/core/store';
import { useConversationRealtimeAudioLevel } from '@/hooks/useConversationRealtimeAudioLevel';
import { VoiceBubble } from './VoiceBubble';
import { conversationMicrophoneService } from '@/services/microphone/ConversationMicrophoneService';
import { directBarsAnimationService, FourBarLevels } from '@/services/voice/DirectBarsAnimationService';
import { sttService } from '@/services/voice/stt';
import { llmService } from '@/services/llm/chat';
import { v4 as uuidv4 } from 'uuid';
import { supabase } from '@/integrations/supabase/client';
import { motion, AnimatePresence } from 'framer-motion';
import { Mic } from 'lucide-react';

// ğŸ¯ CORE STATE MACHINE: This drives everything
type ConversationState = 'listening' | 'thinking' | 'replying' | 'connecting' | 'establishing';


// ğŸµ UTILITY: Safely close AudioContext with proper state checking
const safelyCloseAudioContext = (audioContext: AudioContext | null): void => {
  if (audioContext && audioContext.state !== 'closed') {
    try {
      audioContext.close();
    } catch (error) {
      console.log('[ConversationOverlay] AudioContext already closed, skipping');
    }
  }
};

export const ConversationOverlay: React.FC = () => {
  const { isConversationOpen, closeConversation } = useConversationUIStore();
  const chat_id = useChatStore((state) => state.chat_id);
  // ğŸ¯ PRIMARY: State machine drives everything
  const [state, setState] = useState<ConversationState>('connecting');
  
  // ğŸµ REALTIME AUDIO LEVEL - Auto-attaches to microphone lifecycle
  const audioLevel = useConversationRealtimeAudioLevel({
    updateIntervalMs: 50, // 20fps for React state updates (smooth but not excessive)
    smoothingFactor: 0.8, // Smooth animations
  });
  
  
  // ğŸ¯ ESSENTIAL: Only what we need for state transitions
  const hasStarted = useRef(false);
  const isShuttingDown = useRef(false);
  const connectionRef = useRef<any>(null);
  const currentTtsSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const isProcessingRef = useRef<boolean>(false);
  const pingIntervalRef = useRef<number | null>(null);
  const animationTimeoutRef = useRef<number | null>(null);
  
  // ğŸµ AUDIO: Single context for all audio
  const audioContextRef = useRef<AudioContext | null>(null);
  
  

  // ğŸ¯ CORE STATE MACHINE: Initialize when overlay opens
  useEffect(() => {
    if (isConversationOpen && chat_id) {
      // State starts as 'connecting' to show start button
    }
    
    return () => {
      if (connectionRef.current) {
        connectionRef.current.unsubscribe();
      }
      // ğŸµ ELEGANT: Use utility function for safe AudioContext cleanup
      safelyCloseAudioContext(audioContextRef.current);
    };
  }, [isConversationOpen, chat_id]);

  // ğŸµ AUDIO: Initialize once, reuse for all audio
  useEffect(() => {
    if (!audioContextRef.current) {
      try {
        // Prefer playback latency to reduce power usage on mobile/desktop when playing TTS
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ latencyHint: 'playback' });
      } catch (e) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      }
    }
    
    return () => {
      // ğŸµ ELEGANT: Use utility function for safe AudioContext cleanup
      safelyCloseAudioContext(audioContextRef.current);
      audioContextRef.current = null;
    };
  }, []);

  // ğŸµ WEB AUDIO API: Real-time analysis with AnalyserNode for authentic bars
  const playAudioImmediately = useCallback(async (audioBytes: number[], text?: string) => {
    if (isShuttingDown.current) return;
    
    try {
      // ğŸ¯ CHECK: Ensure AudioContext is available and running
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        console.log('[ConversationOverlay] ğŸµ AudioContext closed or missing, recreating...');
        try {
          audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ latencyHint: 'playback' });
        } catch (e) {
          audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        }
      }
      
      const audioContext = audioContextRef.current;
      
      // ğŸ¯ CHECK: Ensure AudioContext is running (resume if suspended)
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }
      
      // ğŸ¯ DIRECT: Convert bytes to ArrayBuffer and decode
      const arrayBuffer = new Uint8Array(audioBytes).buffer;
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      // ğŸµ REAL-TIME: Create analyzer for live audio analysis
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256; // Good resolution for 4 bars
      analyser.smoothingTimeConstant = 0.8; // Smooth the data
      
      // ğŸ¯ DIRECT: Create source and connect to analyzer + destination
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(analyser);
      analyser.connect(audioContext.destination);
      
      // ğŸš€ PLAY AUDIO IMMEDIATELY - No waiting for anything else
      source.start(0);
      currentTtsSourceRef.current = source;
      
      // ğŸ¯ STATE DRIVEN: Set replying state
      setState('replying');
      
      // ğŸ”‡ Suspend microphone capture during TTS playback (mutual exclusivity)
      try {
        conversationMicrophoneService.suspendForPlayback();
      } catch (e) {
        console.warn('[ConversationOverlay] Could not suspend mic for playback', e);
      }
      
      // ğŸŒ PAUSE: Pause WebSocket during TTS playback to prevent interference
      if (connectionRef.current && connectionRef.current.state === 'SUBSCRIBED') {
        connectionRef.current.unsubscribe();
        console.log('[ConversationOverlay] ğŸŒ WebSocket paused during TTS playback');
      }
      
      // ğŸµ REAL-TIME: Start bars animation service for live data
      directBarsAnimationService.start();
      
      // ğŸµ REAL-TIME: Live audio analysis loop (25fps for mobile performance)
      const frequencyData = new Uint8Array(analyser.frequencyBinCount);
      const animate = () => {
        if (isShuttingDown.current || !currentTtsSourceRef.current) {
          // ğŸ›‘ CLEANUP: Clear timeout reference when stopping
          animationTimeoutRef.current = null;
          return;
        }
        
        // Get live frequency data from the analyzer
        analyser.getByteFrequencyData(frequencyData);
        
        // ğŸµ REAL-TIME: Calculate overall audio level for synchronized bars
        // Sum all frequency bins to get overall audio intensity
        let totalLevel = 0;
        for (let i = 0; i < frequencyData.length; i++) {
          totalLevel += frequencyData[i];
        }
        const rawLevel = totalLevel / (frequencyData.length * 255); // Normalize to 0-1
        
        // ğŸ¯ FIXED: Scale and clamp to prevent huge bars
        const overallLevel = Math.min(1, Math.max(0.2, rawLevel * 0.8 + 0.2)); // Scale to 0.2-1.0 range
        
        // ğŸ¯ SIMPLIFIED: All bars use the same signal for synchronized movement
        const barLevels: FourBarLevels = [
          overallLevel,  // All bars move together
          overallLevel,  // All bars move together
          overallLevel,  // All bars move together
          overallLevel   // All bars move together
        ];
        
        // Send live data to bars
        directBarsAnimationService.notifyBars(barLevels);
        
        // ğŸ›‘ SECURE: Store timeout ID for proper cleanup
        animationTimeoutRef.current = window.setTimeout(animate, 40);
      };
      
      // Start the real-time animation loop
      animationTimeoutRef.current = window.setTimeout(animate, 40);
      
              // ğŸ¯ STATE DRIVEN: Return to listening when done
        source.onended = () => {
          console.log('[ConversationOverlay] ğŸµ TTS audio finished, returning to listening mode');
          
          // ğŸ›‘ CLEANUP: Cancel animation timeout to prevent CPU leak
          if (animationTimeoutRef.current) {
            clearTimeout(animationTimeoutRef.current);
            animationTimeoutRef.current = null;
          }
          
          // ğŸµ Stop animation service when TTS ends
          directBarsAnimationService.stop();
          
          setState('listening');
         
         // ğŸ”Š Resume microphone capture after playback ends
         try {
           conversationMicrophoneService.resumeAfterPlayback();
         } catch (e) {
           console.warn('[ConversationOverlay] Could not resume mic after playback', e);
         }
         
         // ğŸŒ RESUME: Resume WebSocket after TTS playback ends
         if (connectionRef.current && connectionRef.current.state === 'CLOSED') {
           connectionRef.current.subscribe();
           console.log('[ConversationOverlay] ğŸŒ WebSocket resumed after TTS playback');
         }
         
         // ğŸš¨ CHECK: Only restart microphone if we're not shutting down
         if (!isShuttingDown.current) {
            // ğŸ¤ Restart microphone recording with timing buffer to align VAD with TTS end
            setTimeout(() => {
              if (!isShuttingDown.current) {
                try {
                  conversationMicrophoneService.startRecording();
                  console.log('[ConversationOverlay] ğŸ¤ Microphone recording restarted for next turn (with timing buffer)');
                } catch (error) {
                  console.error('[ConversationOverlay] âŒ Failed to restart microphone recording:', error);
                }
              }
            }, 200); // 200ms buffer for more reliable restart after cleanup
         } else {
           // ğŸš« Shutting down - no auto-restart
           console.log('[ConversationOverlay] ğŸ¤ Shutting down, skipping microphone restart');
         }
       };
      

      
    } catch (error) {
      console.error('[ConversationOverlay] âŒ Web Audio API failed:', error);
      setState('connecting');
    }
  }, []);

  // ğŸ¯ CONNECTION: Persistent WebSocket with pause/resume and ping
  const establishConnection = useCallback(async () => {
    if (!chat_id) return false;
    
    try {
      // ğŸš€ OPTIMIZATION: Create WebSocket once at start, reuse for all turns
      if (!connectionRef.current) {
        const connection = supabase.channel(`conversation:${chat_id}`);
        
        // ğŸ¯ DIRECT: WebSocket â†’ Audio + Real-time Analysis
        connection.on('broadcast', { event: 'tts-ready' }, ({ payload }) => {
          if (payload.audioBytes) {
            playAudioImmediately(payload.audioBytes, payload.text);
          }
        });
        
        connection.subscribe();
        connectionRef.current = connection;
        console.log('[ConversationOverlay] ğŸŒ WebSocket connection established and subscribed');
      } else {
        // ğŸ”„ RESUME: Re-subscribe if already exists but paused
        if (connectionRef.current.state === 'CLOSED') {
          connectionRef.current.subscribe();
          console.log('[ConversationOverlay] ğŸŒ WebSocket connection resumed');
        }
      }
      
      // ğŸ“ PING: Keep WebSocket warm with periodic ping
      if (!pingIntervalRef.current) {
        pingIntervalRef.current = window.setInterval(() => {
          if (connectionRef.current && connectionRef.current.state === 'SUBSCRIBED') {
            // Send a lightweight ping to keep connection alive
            connectionRef.current.send({
              type: 'broadcast',
              event: 'ping',
              payload: { timestamp: Date.now() }
            });
          }
        }, 30000); // Ping every 30 seconds
        console.log('[ConversationOverlay] ğŸ“ WebSocket ping interval started');
      }
      
      return true;
    } catch (error) {
      console.error('[ConversationOverlay] Connection failed:', error);
      return false;
    }
  }, [chat_id, playAudioImmediately]);

  // ğŸµ AUDIO: Ping AudioContext to keep it warm
  const pingAudioContext = useCallback(async () => {
    if (!audioContextRef.current) {
      try {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ latencyHint: 'playback' });
        console.log('[ConversationOverlay] ğŸµ AudioContext created');
      } catch (e) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        console.log('[ConversationOverlay] ğŸµ AudioContext created (fallback)');
      }
    }
    
    // ğŸ“ PING: Resume AudioContext if suspended to keep it warm
    if (audioContextRef.current.state === 'suspended') {
      await audioContextRef.current.resume();
      console.log('[ConversationOverlay] ğŸµ AudioContext pinged and resumed');
    } else {
      console.log(`[ConversationOverlay] ğŸµ AudioContext pinged (state: ${audioContextRef.current.state})`);
    }
    
    return audioContextRef.current;
  }, []);

  // ğŸ¯ START: Initialize conversation with warm connections
  const handleStart = useCallback(async () => {
    if (hasStarted.current) return;
    if (!chat_id) return;
    
    hasStarted.current = true;
    
    try {
      // ğŸ¯ STATE DRIVEN: Get microphone
      setState('establishing');
      
      // ğŸš€ WARM START: Initialize WebSocket and AudioContext immediately
      console.log('[ConversationOverlay] ğŸš€ Warming up connections...');
      
      await establishConnection();
      await pingAudioContext();
      
      console.log('[ConversationOverlay] ğŸš€ Connections warmed up and ready');
      
      // ğŸ¯ STATE DRIVEN: Get microphone
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      
      // ğŸ¯ STATE DRIVEN: Cache the stream for the microphone service
      conversationMicrophoneService.cacheStream(stream);
      
      // ğŸ¯ STATE DRIVEN: Initialize microphone service BEFORE starting recording
      conversationMicrophoneService.initialize({
        onRecordingComplete: (audioBlob: Blob) => {
          // Process recording when callback fires
          processRecording(audioBlob);
        },
        onError: (error: Error) => {
          console.error('[ConversationOverlay] Microphone error:', error);
          setState('connecting');
        },
        onSilenceDetected: () => {
          conversationMicrophoneService.stopRecording();
        },
        silenceTimeoutMs: 1200,
      });
      
      // ğŸ¯ STATE DRIVEN: Start recording AFTER initialization
      console.log('[ConversationOverlay] ğŸ¤ Starting recording...');
      const recordingStarted = await conversationMicrophoneService.startRecording();
      console.log('[ConversationOverlay] ğŸ¤ Recording started:', recordingStarted);
      
      if (recordingStarted) {
        setState('listening');
        console.log('[ConversationOverlay] ğŸ¤ State set to listening, ready for voice input');
      } else {
        console.error('[ConversationOverlay] âŒ Failed to start recording');
        setState('connecting');
      }
      
    } catch (error) {
      console.error('[ConversationOverlay] Start failed:', error);
      setState('connecting');
    }
  }, [chat_id, establishConnection, pingAudioContext]);

  // ğŸ¯ PROCESSING: Handle recording completion
  const processRecording = useCallback(async (audioBlob: Blob) => {
    if (!chat_id) {
      console.error('[ConversationOverlay] âŒ No chat_id available for processing');
      return;
    }
    
    // ğŸ›¡ï¸ PROTECTION: Prevent duplicate processing
    if (isProcessingRef.current || state === 'thinking' || state === 'replying') {
      console.log('[ConversationOverlay] ğŸ›¡ï¸ Already processing, ignoring duplicate call');
      return;
    }
    
    isProcessingRef.current = true;
    
    try {
      // ğŸ¯ STATE DRIVEN: Processing state - show thinking UI immediately
      setState('thinking');
      
      // ğŸš€ FIRE-AND-FORGET: Start STT transcription without waiting
      console.log('[ConversationOverlay] ğŸš€ Starting STT transcription (fire-and-forget)');
      sttService.transcribe(audioBlob, chat_id, {}, 'conversation', chat_id)
        .then(result => {
          const transcript = result.transcript?.trim();
          
          if (!transcript) {
            console.log('[ConversationOverlay] âš ï¸ Empty transcript, returning to listening');
            setState('listening');
            return;
          }
          
          console.log('[ConversationOverlay] ğŸ“ Transcript received:', transcript);
          
          // ğŸš€ FIRE-AND-FORGET: Send to LLM without waiting (TTS will come via WebSocket)
          console.log('[ConversationOverlay] ğŸ¯ Calling llmService.sendMessage for TTS generation (fire-and-forget)');
          llmService.sendMessage({
            chat_id,
            text: transcript,
            client_msg_id: uuidv4(),
            mode: 'conversation'
          }).then(() => {
            console.log('[ConversationOverlay] âœ… llmService.sendMessage completed');
            // ğŸ¯ NOTE: Replying state will be set when WebSocket receives TTS audio
          }).catch(error => {
            console.error('[ConversationOverlay] âŒ LLM processing failed:', error);
            setState('connecting');
          });
          
          // ğŸ¯ KEEP THINKING: Don't set replying until we have actual audio from WebSocket
        })
        .catch(error => {
          console.error('[ConversationOverlay] âŒ STT or LLM processing failed:', error);
          setState('connecting');
        });
      
      // ğŸ¯ KEEP THINKING STATE: Don't override thinking state - let the promise chain handle state transitions
      
    } catch (error) {
      console.error('[ConversationOverlay] âŒ Processing failed:', error);
      setState('connecting');
    } finally {
      // ğŸ›¡ï¸ CLEANUP: Reset processing flag
      isProcessingRef.current = false;
    }
  }, [chat_id, establishConnection, state]);

  // ğŸ¯ CLEANUP: Master shutdown - Browser APIs FIRST, then everything else
  const handleModalClose = useCallback(async () => {
    console.log('[ConversationOverlay] ğŸš¨ X Button pressed - Master shutdown starting');
    isShuttingDown.current = true;
    
    // ğŸ›‘ STEP 0: Cancel animation timeout to prevent CPU leak
    if (animationTimeoutRef.current) {
      clearTimeout(animationTimeoutRef.current);
      animationTimeoutRef.current = null;
      console.log('[ConversationOverlay] ğŸ›‘ Animation timeout cancelled');
    }
    
    // ğŸµ STEP 1: Stop Web Audio API source
    if (currentTtsSourceRef.current) {
      try {
        (currentTtsSourceRef.current as AudioBufferSourceNode).stop();
        console.log('[ConversationOverlay] ğŸµ Web Audio API source stopped');
      } catch (e) {
        console.warn('[ConversationOverlay] Could not stop Web Audio API source:', e);
      }
      currentTtsSourceRef.current = null;
    }
    
    // ğŸµ STEP 2: Close AudioContext (browser API)
    safelyCloseAudioContext(audioContextRef.current);
    audioContextRef.current = null;
    console.log('[ConversationOverlay] ğŸµ AudioContext closed');
    
    // ğŸ¤ STEP 3: Stop microphone and release MediaStream (browser API)
    try {
      conversationMicrophoneService.stopRecording();
      conversationMicrophoneService.cleanup();
      console.log('[ConversationOverlay] ğŸ¤ Microphone and MediaStream released');
    } catch (e) {
      console.warn('[ConversationOverlay] Could not cleanup microphone:', e);
    }
    
    // ğŸŒ STEP 4: Close WebSocket connection and ping interval
    if (pingIntervalRef.current) {
      clearInterval(pingIntervalRef.current);
      pingIntervalRef.current = null;
      console.log('[ConversationOverlay] ğŸ“ WebSocket ping interval cleared');
    }
    
    if (connectionRef.current) {
      connectionRef.current.unsubscribe();
      connectionRef.current = null;
      console.log('[ConversationOverlay] ğŸŒ WebSocket connection closed');
    }
    
    // ğŸ¨ STEP 5: Stop animation service
    directBarsAnimationService.stop();
    console.log('[ConversationOverlay] ğŸ¨ Animation service stopped');
    
    // ğŸ¯ STEP 6: Reset UI state (after all browser APIs are closed)
    setState('connecting');
    hasStarted.current = false;
    isShuttingDown.current = false;
    console.log('[ConversationOverlay] ğŸ¯ UI state reset');
    
    closeConversation();
  }, [closeConversation]);


  // ğŸ¯ MICROPHONE: Service is now initialized in handleStart, no need for separate useEffect

  // ğŸ¯ SSR GUARD
  if (!isConversationOpen || typeof document === 'undefined') return null;

  return createPortal(
    <div 
      className="fixed inset-0 z-50 bg-white pt-safe pb-safe"
      data-conversation-overlay
    >
      <div className="h-full w-full flex items-center justify-center px-6">
        {state === 'connecting' ? (
          <div className="text-center text-gray-800 flex flex-col items-center gap-4">
            <div
              className="flex flex-col items-center gap-4 cursor-pointer"
              onClick={handleStart}
            >
              <div className="w-24 h-24 rounded-full bg-gray-100 flex items-center justify-center transition-colors hover:bg-gray-200">
                <Mic className="w-10 h-10 text-gray-600" />
              </div>
              <h2 className="text-2xl font-light">
                Tap to Start Conversation
              </h2>
            </div>
            <button
              onClick={closeConversation}
              aria-label="Close conversation"
              className="w-10 h-10 bg-black rounded-full flex items-center justify-center text-white hover:bg-gray-800 transition-colors"
            >
              âœ•
            </button>
          </div>
        ) : (
          <div className="flex flex-col items-center justify-center gap-6 relative">
            <AnimatePresence mode="wait">
              <motion.div
                key={state}
                initial={{ opacity: 0, scale: 0.98 }}
                animate={{ opacity: 1, scale: 1 }}
                exit={{ opacity: 0, scale: 0.98 }}
                transition={{ duration: 0.2, ease: 'easeInOut' }}
              >
                <VoiceBubble state={state} audioLevel={audioLevel} />
              </motion.div>
            </AnimatePresence>

            <p className="text-gray-500 font-light">
                             {state === 'establishing'
                 ? 'Establishing connectionâ€¦'
                 : state === 'listening'
                 ? 'Listeningâ€¦'
                 : state === 'thinking'
                 ? 'Thinkingâ€¦'
                 : 'Speakingâ€¦'}
            </p>

            <button
              onClick={handleModalClose}
              aria-label="Close conversation"
              className="w-10 h-10 bg-black rounded-full flex items-center justify-center text-white hover:bg-gray-800 transition-colors"
            >
              âœ•
            </button>
          </div>
        )}
      </div>
    </div>,
    document.body
  );
};